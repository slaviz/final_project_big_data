{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52311b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import time\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115be9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-19\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Users/slavi/OneDrive - NVIDIA Corporation/Documents/GitHub/Big-data/spark-3.3.1-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/Users/slavi/OneDrive - NVIDIA Corporation/Documents/GitHub/Big-data/hadoop-3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c97a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36a6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiaiting spark context\n",
    "sc = pyspark.SparkContext(\"local\", appName=\"MillionSongs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c992eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data directly from file\n",
    "spark = SparkSession(sc)\n",
    "df = spark.read.csv(\"MILLION_SONGS.csv\", inferSchema=True, header=True)\n",
    "df = df.drop('transfer_note', 'Unnamed: 0', 'artist_7digitalid', 'artist_mbid', 'artist_mbtags', 'artist_mbtags_count', 'artist_playmeid', 'audio_md5', 'danceability', 'energy', 'release_7digitalid', 'similar_artists', 'song_id', 'track_7digitalid', 'analysis_sample_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71816e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"artist_familiarity\",f.col(\"artist_familiarity\").cast(\"int\"))\n",
    "artist_fam_med = df.approxQuantile(\"artist_familiarity\",[0.5],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e410a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({'artist_familiarity':artist_fam_med[0]})\n",
    "df = df.na.fill({'artist_location':''})\n",
    "df = df.na.fill({'artist_name':''})\n",
    "df = df.na.fill({'release':''})\n",
    "df = df.na.fill({'title':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0772ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Droping NaN for song_hotness\n",
    "df = df.dropna(how='any',subset='song_hotttnesss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c83a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode artist id and release\n",
    "art_indexer = StringIndexer(inputCol='artist_id', outputCol='artist_idIndex')\n",
    "rel_indexer = StringIndexer(inputCol='release', outputCol='releaseIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7124409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = art_indexer.fit(df).transform(df) \n",
    "df = rel_indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a46dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- artist_familiarity: integer (nullable = true)\n",
      " |-- artist_hotttnesss: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: string (nullable = true)\n",
      " |-- artist_longitude: string (nullable = true)\n",
      " |-- artist_location: string (nullable = false)\n",
      " |-- artist_name: string (nullable = false)\n",
      " |-- release: string (nullable = false)\n",
      " |-- song_hotttnesss: string (nullable = true)\n",
      " |-- title: string (nullable = false)\n",
      " |-- artist_terms: string (nullable = true)\n",
      " |-- artist_terms_freq: string (nullable = true)\n",
      " |-- artist_terms_weight: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- end_of_fade_in: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- key_confidence: string (nullable = true)\n",
      " |-- loudness: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- mode_confidence: string (nullable = true)\n",
      " |-- start_of_fade_out: string (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- time_signature_confidence: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- segments_start: string (nullable = true)\n",
      " |-- segments_confidence: string (nullable = true)\n",
      " |-- segments_pitches: string (nullable = true)\n",
      " |-- segments_timbre: string (nullable = true)\n",
      " |-- segments_loudness_max: string (nullable = true)\n",
      " |-- segments_loudness_max_time: string (nullable = true)\n",
      " |-- segments_loudness_start: string (nullable = true)\n",
      " |-- sections_start: string (nullable = true)\n",
      " |-- sections_confidence: string (nullable = true)\n",
      " |-- beats_start: string (nullable = true)\n",
      " |-- beats_confidence: string (nullable = true)\n",
      " |-- bars_start: string (nullable = true)\n",
      " |-- bars_confidence: string (nullable = true)\n",
      " |-- tatums_start: string (nullable = true)\n",
      " |-- tatums_confidence: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- artist_idIndex: double (nullable = false)\n",
      " |-- releaseIndex: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a1d04b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o152.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 40 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1240\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1223\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1224\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1239\u001b[0m )\n\u001b[1;32m-> 1240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o152.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 40 more\r\n"
     ]
    }
   ],
   "source": [
    "df.coalesce(1).write.mode('overwrite').csv('cleaned_data',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23e4f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['song_hotttnesss'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d9eb4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af872d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
